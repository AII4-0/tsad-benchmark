model: Transformer
dataset: NASA-MSL
epochs: 10
batch_size: 256
window_size: 100
num_encoder_layers: 3
num_decoder_layers: 3
dropout: 0.1
prediction_length: 1
learning_rate: 0.001